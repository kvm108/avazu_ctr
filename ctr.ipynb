{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "c:\\users\\kvmura~1\\pychar~1\\avazu_~1\\venv\\lib\\site-packages\\theano\\configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory\n",
      "  warnings.warn(\"DeprecationWarning: there is no c++ compiler.\"\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "c:\\users\\kvmura~1\\pychar~1\\avazu_~1\\venv\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import log_loss, mean_squared_error\n",
    "from sklearn.linear_model import SGDClassifier, LassoCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sknn.mlp import Classifier, Layer\n",
    "from sklearn.metrics import explained_variance_score, accuracy_score\n",
    "from sklearn.model_selection import KFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 24), (1000, 24))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['hour', 'day', 'dow', 'C1', 'banner_pos', 'device_type', 'device_conn_type',\n",
    "            'C14', 'C15', 'C16', 'C17', 'C18', 'C19', 'C20', 'C21', 'site_id', 'site_domain',\n",
    "            'site_category', 'app_id', 'app_domain', 'app_category', 'device_model',\n",
    "            'device_id', 'device_ip']\n",
    "\n",
    "# Load data\n",
    "\n",
    "train = pd.read_csv('data/train-100000R.csv', nrows=1000, dtype={'id': pd.np.string_})\n",
    "test = pd.read_csv('data/train-100000R.csv', nrows=1000, dtype={'id': pd.np.string_})\n",
    "    \n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 24), (1000, 24))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pre-processing non-number values\n",
    "le = LabelEncoder()\n",
    "for col in ['site_id', 'site_domain', 'site_category', 'app_id', 'app_domain', 'app_category', 'device_model',\n",
    "            'device_id', 'device_ip']:\n",
    "    le.fit(list(train[col]) + list(test[col]))\n",
    "    train[col] = le.transform(train[col])\n",
    "    test[col] = le.transform(test[col])\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kvmura~1\\pychar~1\\avazu_~1\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1000, 24),\n",
       " (1000, 24),\n",
       " Index(['id', 'click', 'hour', 'C1', 'banner_pos', 'site_id', 'site_domain',\n",
       "        'site_category', 'app_id', 'app_domain', 'app_category', 'device_id',\n",
       "        'device_ip', 'device_model', 'device_type', 'device_conn_type', 'C14',\n",
       "        'C15', 'C16', 'C17', 'C18', 'C19', 'C20', 'C21'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stochastic Gradient Descent is sensitive to feature scaling, so it is highly recommended to scale your data.\n",
    "scaler = StandardScaler()\n",
    "for col in ['C1', 'banner_pos', 'device_type', 'device_conn_type', 'C14', 'C15', 'C16', 'C17', 'C18', 'C19', 'C20', 'C21']:\n",
    "    ch = np.hstack([train[col], test[col]]).reshape(-1, 1)\n",
    "#     print(ch.shape,'\\n================\\n', ch[:10], '\\n================\\n', ch_reshape.shape, '\\n',ch_reshape[:10])\n",
    "    scaler.fit(ch)\n",
    "    train[col] = scaler.transform(train[col].values.reshape(-1, 1))\n",
    "    test[col] = scaler.transform(test[col].values.reshape(-1, 1))\n",
    "    \n",
    "train.shape, test.shape, train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 26),\n",
       " (1000, 26),\n",
       " pandas.core.frame.DataFrame,\n",
       " pandas.core.frame.DataFrame)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add new features:\n",
    "train['day'] = train['hour'].apply(lambda x: (x - x % 10000) / 1000000)  # day\n",
    "train['dow'] = train['hour'].apply(lambda x: ((x - x % 10000) / 1000000) % 7)  # day of week\n",
    "train['hour'] = train['hour'].apply(lambda x: x % 10000 / 100)  # hour\n",
    " \n",
    "test['day'] = test['hour'].apply(lambda x: (x - x % 10000) / 1000000)  # day\n",
    "test['dow'] = test['hour'].apply(lambda x: ((x - x % 10000) / 1000000) % 7)  # day of week\n",
    "test['hour'] = test['hour'].apply(lambda x: x % 10000 / 100)  # hour\n",
    "\n",
    "train.shape, test.shape, type(train), type(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 26), (1000, 26))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove outliner\n",
    "for col in ['C18', 'C20', 'C21']:\n",
    "    # keep only the ones that are within +3 to -3 standard deviations in the column col,\n",
    "    train = train[np.abs(train[col] - train[col].mean()) <= (3 * train[col].std())]\n",
    "       \n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " KNeighborsClassifier(algorithm='auto', leaf_size=100, metric='minkowski',\n",
       "            metric_params=None, n_jobs=1, n_neighbors=100, p=2,\n",
       "            weights='uniform'),\n",
       " LinearDiscriminantAnalysis(n_components=3, priors=None, shrinkage=None,\n",
       "               solver='svd', store_covariance=False, tol=0.0001),\n",
       " GaussianNB(priors=None),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "             splitter='best'),\n",
       " GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "               learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "               presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "               warm_start=False),\n",
       " SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "        eta0=1e-10, fit_intercept=True, l1_ratio=0.15,\n",
       "        learning_rate='invscaling', loss='log', max_iter=None, n_iter=30,\n",
       "        n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
       "        shuffle=True, tol=None, verbose=5, warm_start=False),\n",
       " ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       " RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "             oob_score=False, random_state=None, verbose=0,\n",
       "             warm_start=False)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define classifiers\n",
    "\n",
    "classifiers = [\n",
    "    LogisticRegression(random_state=0),\n",
    "#     LassoCV(),\n",
    "    KNeighborsClassifier(n_neighbors=100, weights='uniform', algorithm='auto',\n",
    "                         leaf_size=100, p=2, metric='minkowski'),\n",
    "    LinearDiscriminantAnalysis(n_components=3),\n",
    "    GaussianNB(),\n",
    "    DecisionTreeClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    SGDClassifier(loss='log', n_iter=30, verbose=5, learning_rate='invscaling', eta0=0.0000000001), #'invscaling'\n",
    "#     Classifier(\n",
    "#         layers=[\n",
    "# #             Layer('Rectifier', units=100),\n",
    "#             Layer(\"Softmax\")],\n",
    "#         learning_rate=0.0001000,\n",
    "#         learning_rule='momentum',\n",
    "#         learning_momentum=0.9,\n",
    "#         batch_size=25,\n",
    "#         valid_size=0.1,\n",
    "#         # valid_set=(X_test, y_test),\n",
    "#         n_stable=10,\n",
    "#         n_iter=10,\n",
    "#         verbose=True),\n",
    "    ExtraTreesClassifier(n_estimators=100),\n",
    "    RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "#         TODO : https://github.com/dmlc/xgboost/issues/2334\n",
    "#         XGBClassifier(n_estimators=512, max_depth=4),\n",
    "]\n",
    "\n",
    "classifiers    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================================================\n",
      "cvs shape (2,)\n",
      "cv score [[9.    2.   ]\n",
      " [0.814 0.828]]\n",
      "LogisticRegression \n",
      " -> Training time: 3.2990105152130127\n",
      "=====================================================================================================\n",
      "=====================================================================================================\n",
      "cvs shape (2,)\n",
      "cv score [[9.    2.   ]\n",
      " [0.814 0.828]\n",
      " [0.828 0.836]]\n",
      "KNeighborsClassifier \n",
      " -> Training time: 3.0221245288848877\n",
      "=====================================================================================================\n",
      "=====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kvmura~1\\pychar~1\\avazu_~1\\venv\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cvs shape (2,)\n",
      "cv score [[9.    2.   ]\n",
      " [0.814 0.828]\n",
      " [0.828 0.836]\n",
      " [0.828 0.808]]\n",
      "LinearDiscriminantAnalysis \n",
      " -> Training time: 2.974665880203247\n",
      "=====================================================================================================\n",
      "=====================================================================================================\n",
      "cvs shape (2,)\n",
      "cv score [[9.    2.   ]\n",
      " [0.814 0.828]\n",
      " [0.828 0.836]\n",
      " [0.828 0.808]\n",
      " [0.586 0.468]]\n",
      "GaussianNB \n",
      " -> Training time: 2.885432481765747\n",
      "=====================================================================================================\n",
      "=====================================================================================================\n",
      "cvs shape (2,)\n",
      "cv score [[9.    2.   ]\n",
      " [0.814 0.828]\n",
      " [0.828 0.836]\n",
      " [0.828 0.808]\n",
      " [0.586 0.468]\n",
      " [0.72  0.71 ]]\n",
      "DecisionTreeClassifier \n",
      " -> Training time: 2.9995338916778564\n",
      "=====================================================================================================\n",
      "=====================================================================================================\n",
      "cvs shape (2,)\n",
      "cv score [[9.    2.   ]\n",
      " [0.814 0.828]\n",
      " [0.828 0.836]\n",
      " [0.828 0.808]\n",
      " [0.586 0.468]\n",
      " [0.72  0.71 ]\n",
      " [0.802 0.82 ]]\n",
      "GradientBoostingClassifier \n",
      " -> Training time: 3.5013458728790283\n",
      "=====================================================================================================\n",
      "=====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kvmura~1\\pychar~1\\avazu_~1\\venv\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.00, NNZs: 24, Bias: -0.000000, T: 1000, Avg. loss: 0.693033\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.00, NNZs: 24, Bias: -0.000000, T: 2000, Avg. loss: 0.692935\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.00, NNZs: 24, Bias: -0.000000, T: 3000, Avg. loss: 0.692870\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.00, NNZs: 24, Bias: -0.000000, T: 4000, Avg. loss: 0.692818\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.00, NNZs: 24, Bias: -0.000000, T: 5000, Avg. loss: 0.692773\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.00, NNZs: 24, Bias: -0.000000, T: 6000, Avg. loss: 0.692733\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.00, NNZs: 24, Bias: -0.000000, T: 7000, Avg. loss: 0.692697\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.00, NNZs: 24, Bias: -0.000000, T: 8000, Avg. loss: 0.692663\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.00, NNZs: 24, Bias: -0.000000, T: 9000, Avg. loss: 0.692631\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.00, NNZs: 24, Bias: -0.000000, T: 10000, Avg. loss: 0.692601\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 0.00, NNZs: 24, Bias: -0.000000, T: 11000, Avg. loss: 0.692573\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 0.00, NNZs: 24, Bias: -0.000000, T: 12000, Avg. loss: 0.692546\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 0.00, NNZs: 24, Bias: -0.000000, T: 13000, Avg. loss: 0.692520\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 0.00, NNZs: 24, Bias: -0.000000, T: 14000, Avg. loss: 0.692495\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 0.00, NNZs: 24, Bias: -0.000000, T: 15000, Avg. loss: 0.692471\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 0.00, NNZs: 24, Bias: -0.000000, T: 16000, Avg. loss: 0.692448\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 0.00, NNZs: 24, Bias: -0.000000, T: 17000, Avg. loss: 0.692426\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 0.00, NNZs: 24, Bias: -0.000000, T: 18000, Avg. loss: 0.692404\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 0.00, NNZs: 24, Bias: -0.000000, T: 19000, Avg. loss: 0.692383\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 0.00, NNZs: 24, Bias: -0.000000, T: 20000, Avg. loss: 0.692363\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 0.00, NNZs: 24, Bias: -0.000000, T: 21000, Avg. loss: 0.692343\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 0.00, NNZs: 24, Bias: -0.000000, T: 22000, Avg. loss: 0.692323\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 0.00, NNZs: 24, Bias: -0.000000, T: 23000, Avg. loss: 0.692304\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 0.00, NNZs: 24, Bias: -0.000000, T: 24000, Avg. loss: 0.692285\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 0.00, NNZs: 24, Bias: -0.000000, T: 25000, Avg. loss: 0.692267\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 0.00, NNZs: 24, Bias: -0.000000, T: 26000, Avg. loss: 0.692249\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 0.00, NNZs: 24, Bias: -0.000000, T: 27000, Avg. loss: 0.692232\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 0.00, NNZs: 24, Bias: -0.000000, T: 28000, Avg. loss: 0.692215\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 0.00, NNZs: 24, Bias: -0.000000, T: 29000, Avg. loss: 0.692198\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 0.00, NNZs: 24, Bias: -0.000000, T: 30000, Avg. loss: 0.692181\n",
      "Total training time: 0.25 seconds.\n",
      "cv score [[9.    2.   ]\n",
      " [0.814 0.828]\n",
      " [0.828 0.836]\n",
      " [0.828 0.808]\n",
      " [0.586 0.468]\n",
      " [0.72  0.71 ]\n",
      " [0.802 0.82 ]]\n",
      "SGDClassifier \n",
      " -> Training time: 0.2626473903656006\n",
      "=====================================================================================================\n",
      "=====================================================================================================\n",
      "cvs shape (2,)\n",
      "cv score [[9.    2.   ]\n",
      " [0.814 0.828]\n",
      " [0.828 0.836]\n",
      " [0.828 0.808]\n",
      " [0.586 0.468]\n",
      " [0.72  0.71 ]\n",
      " [0.802 0.82 ]\n",
      " [0.806 0.824]]\n",
      "ExtraTreesClassifier \n",
      " -> Training time: 3.9500389099121094\n",
      "=====================================================================================================\n",
      "=====================================================================================================\n",
      "cvs shape (2,)\n",
      "cv score [[9.    2.   ]\n",
      " [0.814 0.828]\n",
      " [0.828 0.836]\n",
      " [0.828 0.808]\n",
      " [0.586 0.468]\n",
      " [0.72  0.71 ]\n",
      " [0.802 0.82 ]\n",
      " [0.806 0.824]\n",
      " [0.82  0.824]]\n",
      "RandomForestClassifier \n",
      " -> Training time: 3.810330867767334\n",
      "=====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.2990105152130127,\n",
       " 3.0221245288848877,\n",
       " 2.974665880203247,\n",
       " 2.885432481765747,\n",
       " 2.9995338916778564,\n",
       " 3.5013458728790283,\n",
       " 0.2626473903656006,\n",
       " 3.9500389099121094,\n",
       " 3.810330867767334]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train - kfold diverging for NN\n",
    "time_taken = []\n",
    "k_fold_splits = 2\n",
    "cv_score = np.array((len(classifiers), k_fold_splits))\n",
    "\n",
    "for classifier in classifiers:\n",
    "    print(\"=====================================================================================================\")\n",
    "    start = time.time()\n",
    "    \n",
    "    if classifier.__class__.__name__ not in ['Classifier', 'SGDClassifier']: #not NN\n",
    "        k_fold = KFold(n_splits=k_fold_splits)\n",
    "        for X_train_idx, X_test_idx in k_fold.split(train):\n",
    "            classifier.fit(train[features], train.click).score(train[features], train.click)\n",
    "        cvs = cross_val_score(classifier, train[features], train.click, cv=k_fold, n_jobs=-1) #all cpu's\n",
    "        print(\"cvs shape\", cvs.shape)\n",
    "        cv_score = np.vstack((cv_score, cvs))\n",
    "            \n",
    "    \n",
    "    else:\n",
    "        classifier.fit(train[features], train.click)\n",
    "        cvs = np.zeros((k_fold_splits))\n",
    "    \n",
    "    print(\"cv score\", cv_score)\n",
    "    time_taken.append(time.time() - start)\n",
    "    print('{} \\n -> Training time: {}'.format(classifier.__class__.__name__, time.time() - start))\n",
    "    print(\"=====================================================================================================\")\n",
    "    \n",
    "    \n",
    "time_taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier                 | Log loss                  | Rmse loss                 | Accuracy   | Time\n",
      "LogisticRegression         | 0.40144727326001545       | 0.3537067386221846        | 0.834      | 3.044811725616455\n",
      "KNeighborsClassifier       | 0.44502315158365513       | 0.3711123819006852        | 0.832      | 2.9766688346862793\n",
      "LinearDiscriminantAnalysis | 0.6145220917464038        | 0.3919325348491192        | 0.832      | 3.0550663471221924\n",
      "GaussianNB                 | 1.4593109756307105        | 0.5568927816905266        | 0.555      | 2.871941566467285\n",
      "DecisionTreeClassifier     | 0.002772588722240776      | 0.03162277660168379       | 0.998      | 2.9717466831207275\n",
      "GradientBoostingClassifier | 0.2709133653722514        | 0.28197951622968914       | 0.891      | 3.4896512031555176\n",
      "SGDClassifier              | 0.6921718115546823        | 0.49951207800750835       | 0.832      | 0.33983302116394043\n",
      "ExtraTreesClassifier       | 0.002772588722240776      | 0.03162277660168379       | 0.998      | 4.007874250411987\n",
      "RandomForestClassifier     | 0.1051912910443653        | 0.14067380298941828       | 0.998      | 3.9257187843322754\n"
     ]
    }
   ],
   "source": [
    "# Evaluation \n",
    "\n",
    "log_losses = []\n",
    "rmse_loss = []\n",
    "accuracy = []\n",
    "y_expected = test.click.values\n",
    "\n",
    "print('{:<26} | {:<25} | {:<25} | {:<10} | {}'.format('Classifier', 'Log loss', 'Rmse loss', 'Accuracy', 'Time'))\n",
    "\n",
    "for idx, classifier in enumerate(classifiers):\n",
    "    if classifier.__class__.__name__ != 'LassoCV':\n",
    "        y_predicted = classifier.predict_proba(test[features])\n",
    "        accu_score = accuracy_score(y_expected, classifier.predict(test[features]))\n",
    "    log_loss_class = log_loss(y_expected, y_predicted)\n",
    "    rmse_loss_class = mean_squared_error(y_expected, np.compress([False, True], y_predicted, axis=1))**0.5\n",
    "    \n",
    "    \n",
    "    log_losses.append(log_loss_class)\n",
    "    rmse_loss.append(rmse_loss_class)\n",
    "    accuracy.append(accu_score)\n",
    "\n",
    "    print('{:<26} | {:<25} | {:<25} | {:<10} | {}'.format(classifier.__class__.__name__, log_loss_class, rmse_loss_class, accu_score, time_taken[idx]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best\n",
      "Log loss (Lowest)\n",
      "  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "Rmse (Lowest) \n",
      "  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "Accuracy (Highest)\n",
      " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "Time (Least)\n",
      " SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=1e-10, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='invscaling', loss='log', max_iter=None, n_iter=30,\n",
      "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
      "       shuffle=True, tol=None, verbose=5, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(\"Best\")\n",
    "print('Log loss (Lowest)\\n ', classifiers[log_losses.index(min(log_losses))])\n",
    "print('Rmse (Lowest) \\n ', classifiers[rmse_loss.index(min(rmse_loss))])\n",
    "print('Accuracy (Highest)\\n', classifiers[accuracy.index(max(accuracy))])\n",
    "print('Time (Least)\\n', classifiers[time_taken.index(min(time_taken))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: \n",
    "# 1. Grid search \n",
    "# 2. Plot this to see time vs accuracy vs error\n",
    "# 3. nn and sgd parameter\n",
    "# 4. Integrate the feature selection part - choose best feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
